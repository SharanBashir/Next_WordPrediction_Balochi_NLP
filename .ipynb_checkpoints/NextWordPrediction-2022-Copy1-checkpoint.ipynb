{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3185,
     "status": "ok",
     "timestamp": 1721079366927,
     "user": {
      "displayName": "Sharan Bashir",
      "userId": "13540675494231160315"
     },
     "user_tz": -60
    },
    "id": "V8Ju9JvOE1UD",
    "outputId": "084d8e7a-e7ce-468a-c84b-c135f3843a3f"
   },
   "outputs": [],
   "source": [
    "import os, nltk\n",
    "import requests\n",
    "import io #codecs\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk import word_tokenize, sent_tokenize, bigrams,trigrams\n",
    "import pandas as pd\n",
    "import codecs, re\n",
    "\n",
    "# # Ensure necessary NLTK data packages are downloaded\n",
    "# nltk.download('punkt_tab')\n",
    "\n",
    "# Step 1: Load the Balochi corpus from a text file\n",
    "if os.path.isfile('balochi.txt'):\n",
    "    with io.open('balochi.txt', encoding='utf8') as fin:\n",
    "        text = fin.read()\n",
    "else:\n",
    "    print('file not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8925,
     "status": "ok",
     "timestamp": 1721079379688,
     "user": {
      "displayName": "Sharan Bashir",
      "userId": "13540675494231160315"
     },
     "user_tz": -60
    },
    "id": "Llt6gzNDFKmX",
    "outputId": "8a6d14ee-3e1b-438b-a5cc-43890ddee531"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317181"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of words in corpus\n",
    "len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 225,
     "status": "ok",
     "timestamp": 1721079383843,
     "user": {
      "displayName": "Sharan Bashir",
      "userId": "13540675494231160315"
     },
     "user_tz": -60
    },
    "id": "1eLfvpPwE1UI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253652\n",
      "63530\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Split the text into training and testing sets (80% train, 20% test)\n",
    "split = int(0.8 * len(text))\n",
    "\n",
    "train = text[:split]\n",
    "test = text[split:]\n",
    "\n",
    "print(len(train.split()))\n",
    "print(len(test.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1721079387353,
     "user": {
      "displayName": "Sharan Bashir",
      "userId": "13540675494231160315"
     },
     "user_tz": -60
    },
    "id": "s0DJedTME1UJ",
    "outputId": "03d66077-b5f9-47e3-d6f8-c9174dbbd661"
   },
   "outputs": [],
   "source": [
    "# Step 3: Split texts into sentences using '۔' as the delimiter\n",
    "train_sentences = train.split('۔')\n",
    "test_sentences = test.split('۔')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3327,
     "status": "ok",
     "timestamp": 1721079406390,
     "user": {
      "displayName": "Sharan Bashir",
      "userId": "13540675494231160315"
     },
     "user_tz": -60
    },
    "id": "eHe6nIyRE1UK",
    "outputId": "63110e8f-538d-49cf-b519-119ea28b6865"
   },
   "outputs": [],
   "source": [
    "# # Step 3: Define a function to clean and tokenize text\n",
    "\n",
    "# # Training text\n",
    "# # Remove unwanted characters and digits from corpus as our file is already cleaned then we don't need this one\n",
    "#  tokenized_text = [train_sentences.translate(str.maketrans('', '', '''’؟':؛،”“\"…‘0123456789><*&,^%$#@!_.-:;?|\\/][)(}{+='\"''')) for sent in sentences]\n",
    "\n",
    "# Step 4: Tokenize sentences using WhitespaceTokenizer\n",
    "\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "# This tokenizer splits sentences into words based on whitespace\n",
    "\n",
    "tokenized_train = [tokenizer.tokenize(sentence) for sentence in train_sentences if sentence.strip()]\n",
    "tokenized_test = [tokenizer.tokenize(sentence) for sentence in test_sentences if sentence.strip()]\n",
    "\n",
    "\n",
    "# tokenized_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 1001,
     "status": "ok",
     "timestamp": 1721079426483,
     "user": {
      "displayName": "Sharan Bashir",
      "userId": "13540675494231160315"
     },
     "user_tz": -60
    },
    "id": "rcIhx51kE1UM"
   },
   "outputs": [],
   "source": [
    "# tokenized_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 339,
     "status": "ok",
     "timestamp": 1721079429337,
     "user": {
      "displayName": "Sharan Bashir",
      "userId": "13540675494231160315"
     },
     "user_tz": -60
    },
    "id": "frR1Thw3E1UN"
   },
   "outputs": [],
   "source": [
    "# Step 5: Prepare the data for the N-gram model\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "n = 3  # Trigram model\n",
    "# Generate padded n-grams and corresponding padded sentences\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_train)\n",
    "test_data, padded_test = padded_everygram_pipeline(n, tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 339,
     "status": "ok",
     "timestamp": 1721079429337,
     "user": {
      "displayName": "Sharan Bashir",
      "userId": "13540675494231160315"
     },
     "user_tz": -60
    },
    "id": "frR1Thw3E1UN"
   },
   "outputs": [],
   "source": [
    "# Convert generators to lists for multiple iterations\n",
    "train_data = list(train_data)\n",
    "padded_sents = list(padded_sents)\n",
    "test_data = list(test_data)\n",
    "padded_test = list(padded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1721079432084,
     "user": {
      "displayName": "Sharan Bashir",
      "userId": "13540675494231160315"
     },
     "user_tz": -60
    },
    "id": "6_R7RWw5E1UO"
   },
   "outputs": [],
   "source": [
    "# Step 6: Initialize and train the Lidstone N-gram model\n",
    "from nltk.lm import Laplace, KneserNeyInterpolated, Lidstone\n",
    "\n",
    "# Lidstone smoothing with gamma=0.5 helps handle zero probabilities for unseen n-grams\n",
    "model = Lidstone(0.5, n)\n",
    "model.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8047,
     "status": "ok",
     "timestamp": 1721079446234,
     "user": {
      "displayName": "Sharan Bashir",
      "userId": "13540675494231160315"
     },
     "user_tz": -60
    },
    "id": "a9FdfHmGE1UQ",
    "outputId": "1d26cac4-6458-4770-eded-0463971666e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10268\n",
      "Ngram counts: <NgramCounter with 3 ngram orders and 993936 ngrams>\n"
     ]
    }
   ],
   "source": [
    "# Print vocab and stats new\n",
    "print(\"Vocabulary size:\", len(model.vocab))\n",
    "print(\"Ngram counts:\", model.counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1721079455300,
     "user": {
      "displayName": "Sharan Bashir",
      "userId": "13540675494231160315"
     },
     "user_tz": -60
    },
    "id": "RPgQ5hubE1UR",
    "outputId": "26550685-0839-4d54-9643-1156d2b9f11b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 3 ngram orders and 993936 ngrams>\n"
     ]
    }
   ],
   "source": [
    "#number of unigrams, bigrams, trigrams, 4-grams and so on...\n",
    "print(model.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2175,
     "status": "ok",
     "timestamp": 1721079462746,
     "user": {
      "displayName": "Sharan Bashir",
      "userId": "13540675494231160315"
     },
     "user_tz": -60
    },
    "id": "8WHeT34OE1UR",
    "outputId": "0438f212-0c4f-4812-aca0-1284a060fad3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 313.8818666764716\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Evaluate the model's perplexity on the test data\n",
    "# Perplexity measures how well the model predicts the test data; lower is better\n",
    "total_perplexity = 0\n",
    "for i, test_ngram in enumerate(test_data):\n",
    "    perplexity = model.perplexity(test_ngram)\n",
    "    total_perplexity += perplexity\n",
    "\n",
    "average_perplexity = total_perplexity / (i + 1)\n",
    "print(f\"Perplexity: {average_perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10835,
     "status": "ok",
     "timestamp": 1721079561842,
     "user": {
      "displayName": "Sharan Bashir",
      "userId": "13540675494231160315"
     },
     "user_tz": -60
    },
    "id": "ORw8Jl3yVb2M",
    "outputId": "52f20762-c966-464a-e3cf-b342c7b5b328"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-bidi\n",
      "  Downloading python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from python-bidi) (1.16.0)\n",
      "Installing collected packages: python-bidi\n",
      "Successfully installed python-bidi-0.4.2\n"
     ]
    }
   ],
   "source": [
    "pip install python-bidi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10835,
     "status": "ok",
     "timestamp": 1721079561842,
     "user": {
      "displayName": "Sharan Bashir",
      "userId": "13540675494231160315"
     },
     "user_tz": -60
    },
    "id": "ORw8Jl3yVb2M",
    "outputId": "52f20762-c966-464a-e3cf-b342c7b5b328"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigrams</th>\n",
       "      <th>bigram_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(تہ, ءَ)</td>\n",
       "      <td>3563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(ءِ, تہ)</td>\n",
       "      <td>2415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(سر, ءَ)</td>\n",
       "      <td>1146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(کہ, آ)</td>\n",
       "      <td>1037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(بوتگ, اَت)</td>\n",
       "      <td>888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(وت, ءَ)</td>\n",
       "      <td>692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(ءَ, یک)</td>\n",
       "      <td>672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(ءِ, سر)</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(نہ, اَت)</td>\n",
       "      <td>615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(لوٹ, اِت)</td>\n",
       "      <td>613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bigrams  bigram_count\n",
       "0     (تہ, ءَ)          3563\n",
       "1     (ءِ, تہ)          2415\n",
       "2     (سر, ءَ)          1146\n",
       "3     (کہ, آ)          1037\n",
       "4  (بوتگ, اَت)           888\n",
       "5     (وت, ءَ)           692\n",
       "6     (ءَ, یک)           672\n",
       "7     (ءِ, سر)           633\n",
       "8    (نہ, اَت)           615\n",
       "9   (لوٹ, اِت)           613"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from collections import Counter  # For counting hashable objects\n",
    "from itertools import chain      # For flattening lists\n",
    "from bidi.algorithm import get_display  # For proper display of bidirectional text (e.g., Arabic, Balochi)\n",
    "import pandas as pd  # For creating and handling dataframes\n",
    "\n",
    "# Generate bigrams from the tokenized training data\n",
    "# 'tokenized_train' is assumed to be a list of tokenized sentences (i.e., list of lists of tokens)\n",
    "# For each sentence, zip the sentence with itself offset by one to create bigrams\n",
    "# Then, flatten the list of bigrams from all sentences into a single iterable using chain.from_iterable\n",
    "bigram_fd = Counter(\n",
    "    chain.from_iterable(\n",
    "        zip(sublist, sublist[1:]) for sublist in tokenized_train\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create a DataFrame from the 10 most common bigrams\n",
    "# 'most_common(10)' returns a list of the 10 most frequent bigrams and their counts\n",
    "bigram_df = pd.DataFrame(\n",
    "    bigram_fd.most_common(10),\n",
    "    columns=['bigrams', 'bigram_count']\n",
    ")\n",
    "\n",
    "# Display the DataFrame\n",
    "bigram_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21033,
     "status": "ok",
     "timestamp": 1721079787674,
     "user": {
      "displayName": "Sharan Bashir",
      "userId": "13540675494231160315"
     },
     "user_tz": -60
    },
    "id": "dsTSpA-wE1UW",
    "outputId": "3eab4fdf-fba4-428a-a6e0-a59cf8415a77"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a word:  تو\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('من', 46), ('منی', 36), ('وتی', 35)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = (input('Enter a word: ')).split()\n",
    "sorted(model.counts[input_text].most_common(3))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
